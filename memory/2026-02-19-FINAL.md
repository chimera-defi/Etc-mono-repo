# 2026-02-19 FINAL ‚Äî Benchmark Work Complete Record

## Timeline (Feb 19 Work Stream)

- **15:34** ‚Äî LFM2.5-1.2B atomic complete: **11/12 (95.55% Agent Score)**
- **17:33** ‚Äî Quick validation: gpt-oss (3/3 sampled), mistral (2/3 sampled)
- **18:25** ‚Äî Mistral full atomic: **8/12 (66.7% Agent Score)**
- **21:46** ‚Äî Mistral extended phase: **8/18 (44.4% Agent Score)**
- **21:53** ‚Äî Phase 2 harness spawned (5 models parallel)
- **22:00** ‚Äî gpt-oss retry complete: **5/12 (41.7% Agent Score)**
- **Plus:** Harness design, runner consolidation, documentation updates

---

## Key Decisions Made

### Architecture
- **Use native Ollama tools API** ‚Äî Not JSON parsing from `generate()` output
  - `POST /api/chat` with `tools` parameter for proper tool invocation
  - Enables bracket notation `[tool_name(arg="value")]`, tags, bare JSON
  
### Robustness
- **Signal-based timeout** ‚Äî Corrects gpt-oss hang on P9
  - 60-second per-prompt limit using `signal.alarm()`
  - Graceful timeout vs blocking indefinitely
  
### Test Design
- **Per-model variants** ‚Äî Bracket, safety, conciseness branches
  - Different harness for each model's strengths
  - Phase 2 validates this approach
  
- **Early-exit rules** ‚Äî >50% fail on atomic = skip extended
  - Saves compute for poorly-performing models
  - Applied to gpt-oss and others

### Production Change
- **Will change openclaw.json fallback** ‚Äî qwen ‚Üí LFM2.5-1.2B
  - 95.55% Agent Score vs 62.2% qwen
  - Perfect restraint (1.0) on safety-critical prompts

---

## Results Summary

### Atomic Phase (P1-P12)

| Model | Pass | Total | Score | Restraint | Notes |
|-------|------|-------|-------|-----------|-------|
| **LFM2.5-1.2B** | 11 | 12 | **95.55%** | **1.0** ‚úÖ | Production ready |
| mistral:7b | 8 | 12 | 66.7% | 0.83 | Qualified for extended |
| gpt-oss:latest | 5 | 12 | 41.7% | ? | Retried, timeout issues |
| qwen2.5:3b | 7 | 12 | 62.2% | 0.33 ‚ö†Ô∏è | Too aggressive on P5, P11 |

### Extended Phase (P13-P30)

| Model | Pass | Total | Score | Drop | Status |
|-------|------|-------|-------|------|--------|
| mistral:7b | 8 | 18 | 44.4% | ‚àí22.3% | Extended is hard |

### Phase 2 Harness (5 models parallel, in progress at 22:10 GMT+1)

- Per-model variants (bracket, safety, conciseness)
- Early-exit validation
- Multi-turn conversation patterns
- TBD ‚Äî Will compile once all runs complete

---

## Lessons Learned

### Safety > Format Compliance
- **Restraint is critical:** LFM achieves 1.0 on P5/P11/P12 (refuse unsafe calls)
- Compare to qwen: 0.33 (calls get_weather when told not to, calls schedule_meeting on weather request)
- The real Agent Score: Action (0.4) + Restraint (0.3) + Wrong-Tool-Avoidance (0.3)

### Extended Testing is Hard
- **Atomic to Extended drop:** 66.7% ‚Üí 44.4% for mistral
- Multi-turn conversation > atomic one-shot prompts
- gpt-oss and others didn't make the cut for extended (early-exit triggered)

### Harness Variants Matter
- One-size-fits-all doesn't work
- Phase 2 designed to test per-model approaches
- Different models respond to bracket vs tag vs bare JSON differently

### Multi-Turn > Atomic
- P13-P30 require sustained tool-calling across conversation turns
- Much more realistic than single-prompt validation
- Separates "lucky pass" from "actually reliable"

---

## Technical Assets

### Code
- `/root/.openclaw/workspace/bench/local_tool_calling_benchmark.py` ‚Äî Multi-format parser + Agent Score
- `/root/.openclaw/workspace/bench/tool-calling-benchmark-ref/` ‚Äî Cloned MikeVeerman methodology
- Harness runner consolidation (phase 1 + phase 2 unified)

### Documentation
- `bench/BENCHMARK_RESULTS.md` ‚Äî Concise results (67 lines, essential only)
- This file ‚Äî Complete record for continuity

---

## Next (After Phase 2 Completes)

1. **Compile final PR** ‚Äî All results from atomic, extended, phase 2
   - Branch: `feat/llm-benchmark-final`
   - Single clean commit, no fluff

2. **Lock LFM2.5 as fallback**
   - Production-ready: 95.55%, perfect restraint
   - Replaces qwen in codebase

3. **Update openclaw.json**
   - Register LFM2.5-1.2B as default fallback model
   - Clean up old references

4. **Archive methodology**
   - Document harness approach for future model testing
   - Include lessons learned + variant templates
   - Enable rapid benchmarking of new models (e.g., LFM3, upcoming releases)

---

## Session Status at 22:10 GMT+1

- ‚úÖ Atomic phase complete (all 4 models tested)
- ‚úÖ Extended phase complete (mistral only)
- üîÑ Phase 2 harness: 5 models in parallel, ~30-40 min remaining
- ‚è≥ Final PR compilation: after Phase 2 completes

**Confidence:** High. LFM2.5 is production-ready now. Phase 2 will validate harness variants for future work.

---

## Feb 20 Session ‚Äî Phase 2 Debugging & Fix Implementation

### Issue Encountered
- **Parallel Phase 2 runs (5 models) caused resource contention hangs**
  - LFM2.5, mistral, qwen, gpt-oss all stalled or hit errors
  - Root: CPU bandwidth exhaustion with large models under load
  - Decision: Switch to sequential-only execution

### Root Causes Identified (Debug Agents)
1. **P1-P3 Timeouts (60s)** ‚Äî Signal-based timeout can't interrupt network I/O
   - HTTP client timeout fires first ‚Üí ReadTimeout exception
   - Solution: Replace signal.alarm() with proper network exception handling
2. **P5-P6 NoneType Errors** ‚Äî ollama can return `{"tool_calls": {}}` (dict, not list)
   - Even `(tool_calls or [])` fails when tool_calls is an empty dict
   - Solution: Type-validate each field before use

### Fixes Applied (Committed)
- ‚úÖ Robust parser patch: validates msg is dict, tool_calls is list/tuple, each tool_name is string
- ‚úÖ Type-safe extraction function: graceful fallback to [] on any anomaly
- ‚úÖ 18/18 test cases pass on robust extraction
- ‚úÖ Sequential execution script: one model at a time, 5-second pause between

### Current Status (22:19 GMT+1)
- Sequential batch restarted with corrected harness
- LFM2.5 running first (timeout handling TBD)
- mistral, qwen queued after
- Timeouts are Ollama issue (not harness), separate from parser fix

### Key Learning
**Resource contention + parallel runs = unpredictable results.** This should be documented in BEST_PRACTICES.md for any future benchmarking work. Sequential execution is the only reliable approach for CPU-constrained systems.

### eth2-quickstart Status
- `/root/.openclaw/workspace/dev/eth2-quickstart` ‚Äî 1 commit ahead of master
- Current: ETHGas E2E testing + Caddy security headers validation (Feb 15)
- TODO: takopi integration (part of PR #82)
- Decision: Finish benchmark PR first, circle back to eth2-quickstart takopi after

### Deliverables Ready
- Robust harness fixes in `/tmp/` (from debug agents)
- Consolidated runner + aggregation tools built
- Extended prompt suite (P13-P30, 18 prompts)
- Per-model harness variants documented
- Final PR skeleton ready to populate with real Phase 2 results

### Next Steps
1. Complete sequential Phase 2 run with corrected harness (timeout handling)
2. Compile final consolidated PR with honest assessment
3. Multi-pass review against MikeVeerman patterns
4. Merge benchmark PR, then tackle eth2-quickstart takopi work
