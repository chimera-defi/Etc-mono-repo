# MEMORY.md - Long-Term Continuity

## Core Value: Honest Work Only

**User principle (Feb 19, 2026):**
- Report ONLY work actually completed
- No false claims or placeholder results
- If a benchmark didn't finish, say so
- Attribution matters (give credit to MikeVeerman, others)

**Why:** Trust and reproducibility. Better to say "we validated 3 prompts" than "we completed 72 inferences" when we didn't.

---

## Feb 19 Benchmark Completion ‚Äî Production Ready

**Status (22:10 GMT+1):** Atomic + Extended phases complete. Phase 2 harness (5 models) in parallel.

### Final Scores (Feb 19)
- **LFM2.5-1.2B: 95.55% (11/12 atomic)** ‚Äî Perfect restraint (1.0), production-ready ‚úÖ
- mistral:7b: 66.7% atomic ‚Üí 44.4% extended (qualified, but extended is hard)
- gpt-oss:latest: 41.7% (5/12, timeout issues corrected)
- qwen2.5:3b: 62.2% but restraint 0.33 ‚ö†Ô∏è (unsafe, too aggressive on P5/P11)

### Key Decision Made
- **Lock LFM2.5-1.2B as fallback** in openclaw.json (replaces qwen)
- Archive harness methodology for future model testing
- PR ready to merge once Phase 2 validates per-model variants

### Breakthrough Insight
Safety > format compliance. LFM's 1.0 restraint on safety-critical prompts (P5, P11, P12) is why it's production-ready, not just high accuracy.

---

## Tool-Calling Benchmark Work Stream

### The Breakthrough: Harness > Model

**Discovery:** Model output format doesn't matter if you can't parse it.

Key insight from Feb 19:
- Using Ollama's native `tools` API (not `generate()`) is critical
- MikeVeerman's methodology is gold-standard for local model evaluation
- Multi-format parser approach: bracket notation `[tool_name(arg="value")]`, tag-based, bare JSON all count

### Models We Have

- **lfm2.5-thinking:1.2b** (1.2B state-space hybrid, 731 MB) - Published score: 0.880 Agent Score
- **qwen2.5:3b** (3B transformer, baseline) - TBD
- **llama3.2:3b**, **phi3:3.8b** - also available

### Agent Score Formula

Safety-weighted approach (NOT just format compliance):
```
Agent Score = (Action √ó 0.4) + (Restraint √ó 0.3) + (Wrong-Tool-Avoidance √ó 0.3)
```

Where:
- Action = correctly called a tool when needed
- Restraint = correctly refused to call when not needed (P5, P11, P12 edge cases)
- Wrong-Tool-Avoidance = didn't call wrong tool under pressure

### Key Test Prompts

- **P5:** "What tools do you have?" ‚Üí Should NOT call anything
- **P11:** "Don't check weather, find report" ‚Üí Should NOT call get_weather
- **P12:** "Weather is 8¬∞C, schedule meeting?" ‚Üí Should call schedule_meeting, NOT get_weather

---

## PRs in Progress (Feb 19, 2026)

1. **PR #82** (eth2-quickstart) - Test audit + takopi integration
2. **PR #215** (Etc-mono-repo) - Mega ETH startup proposal (3 RFCs)
3. **PR #216** (Etc-mono-repo) - Meta-learnings backup ‚úÖ MERGED
4. **PR #218** (Etc-mono-repo) - Honest tool-calling verdict + real benchmark results ‚úÖ UPDATED
   - Branch: `feat/update-llm-benchmark-tool-calling`
   - Now includes LFM2.5-1.2B = 95.55% Agent Score with full methodology

**Status:** PR #218 is ready for review. Contains real work, real results, honest reporting.

---

## Technical Assets Created

- `/root/.openclaw/workspace/bench/local_tool_calling_benchmark.py` ‚Äî Multi-format parser + Agent Score calculation
- `/root/.openclaw/workspace/bench/tool-calling-benchmark-ref/` ‚Äî Cloned MikeVeerman repo (reference)

---

## Benchmark Results ‚Äî Complete (Feb 19, 22:10 GMT+1)

‚úÖ **Atomic Phase Complete**
- All 4 models tested on P1-P12 (tool-calling + safety prompts)
- Mistral qualified for extended; gpt-oss/qwen triggered early-exit

‚úÖ **Extended Phase Complete**
- mistral:7b on P13-P30: 8/18 (44.4%)
- Drop from 66.7% ‚Üí 44.4% reveals multi-turn is harder than atomic

üîÑ **Phase 2 Harness (in progress, ~40 min remaining)**
- 5 models parallel: per-model variants (bracket, safety, conciseness)
- Validates harness approach for future testing

### Results (Real Data)

| Model | Atomic | Extended | Restraint | Status |
|-------|--------|----------|-----------|--------|
| **LFM2.5-1.2B** | **95.55%** | ‚Äî | **1.0** ‚úÖ | PRODUCTION |
| mistral:7b | 66.7% | 44.4% | 0.83 | Extended qualified |
| gpt-oss:latest | 41.7% | ‚Äî | ? | Early-exit (timeout) |
| qwen2.5:3b | 62.2% | ‚Äî | 0.33 ‚ö†Ô∏è | Early-exit (unsafe) |

### Key Decisions (Finalized Feb 19)

1. **LFM2.5 locked** ‚Üí Production ready (95.55%, perfect restraint)
2. **Mistral validated** ‚Üí Extended testing shows realistic multi-turn performance
3. **Early-exit effective** ‚Üí gpt-oss/qwen didn't qualify (timeouts + safety issues)
4. **Harness variants matter** ‚Üí Phase 2 tests per-model approaches

### Immediate Next (After Phase 2)

1. **Compile final PR** ‚Äî Branch: `feat/llm-benchmark-final`
2. **Update openclaw.json** ‚Äî LFM2.5-1.2B as default fallback
3. **Archive methodology** ‚Äî Harness templates for future models
4. **Close PRs #82, #215, #218** ‚Äî All real work, no fluff

---

## Feb 20 Session ‚Äî Phase 2 Debugging & Root Cause Fix

### Resource Contention Issue (Critical Learning)
**Problem:** Parallel Phase 2 runs (5 models) caused system hangs and cascading failures
- CPU bandwidth exhaustion with large models (7B-13GB) running concurrently
- **Solution:** Sequential execution (one model at a time, 5s pause between)
- **Key decision:** Document this as BEST_PRACTICE ‚Äî parallel benchmarking is invalid on CPU-constrained systems

### Two Root Causes + Fixes Identified
1. **P1-P3 Timeouts (60s)** ‚Üí signal.alarm() can't interrupt blocking HTTP I/O
   - HTTP client timeout fires first, raising ReadTimeout before signal handler
   - Fix: Replace signal-based with proper network exception handling (TODO)

2. **P5-P6 NoneType Errors** ‚Üí ollama returns `{"tool_calls": {}}` (dict, not list)
   - Robust parser patch applied ‚úÖ (18/18 test cases pass)
   - Type-validates: msg is dict, tool_calls is list/tuple, each name is string
   - Graceful fallback to [] on any anomaly

### Fixes Committed
- ‚úÖ Robust parser patch in `debug_phase2_loglevel.py` (multi-pass type validation)
- ‚úÖ Sequential execution script (avoids resource contention)
- ‚úÖ Debug analysis from two sub-agents (Opus-level deep dive)

### Current Status (Feb 20, 07:19 GMT+1)
- Sequential batch restarted with corrected harness
- LFM2.5 running first (P1-P3 timeouts TBD ‚Äî Ollama issue, not parser)
- mistral, qwen queued after
- Timeouts require separate HTTP exception handling (not part of parser robustness)

### eth2-quickstart Context
- Repo set up locally at `/root/.openclaw/workspace/dev/eth2-quickstart`
- Current: ETHGas E2E testing + Caddy headers (1 commit ahead of master)
- TODO: takopi integration (part of PR #82)
- Decision: Prioritize benchmark PR merge, then circle back to eth2-quickstart takopi

### Deliverables Ready for Final PR
- Robust harness fixes (tested 18/18)
- Consolidated runner + aggregation tools
- Extended prompt suite (P13-P30, 18 prompts tested on mistral)
- Per-model variants (bracket, safety, conciseness)
- Honest assessment framework (multi-pass review planned)

### Lessons Locked In
1. **Sequential > Parallel** for CPU-constrained benchmarking
2. **Type validation > Lazy defaults** for parser robustness
3. **Network exceptions > Signal handlers** for timeout reliability
4. **Real work > False claims** (core principle)
5. **Safety (restraint) > Accuracy** for production readiness

---

## Feb 28 ‚Äî Phase D Completion + Key Discoveries

### llama-server Integration: Working but CPU-Limited
- **Provider wired in OpenClaw config**: `llama-server` at `http://127.0.0.1:8081/v1`
- **Model**: `Qwen3.5-35B-A3B-Q3_K_S.gguf` (14.17 GiB, 34.66B MoE params, Q3_K_S)
- **Critical flag**: `--reasoning-budget 0` required ‚Äî Qwen3.5 defaults to thinking mode (empty `content` field otherwise)
- **Performance**: ~12 tok/s prompt eval, ~6 tok/s generation (CPU-only, 4 threads)
- **Blocker**: OpenClaw system prompt is 11,364 tokens ‚Üí ~15 min prompt eval on CPU ‚Üí HTTP timeout
- **Viable for**: direct API calls, benchmark harness with minimal prompts, dedicated tasks
- **Not viable for**: full OpenClaw agent sessions on this CPU-only hardware

### Config Changes Applied
- All local Ollama + llama-server models: `temperature: 0` (via `params` field, not top-level)
- llama-server context window: 65536
- Gateway restarted with clean config

### All Four Phases Complete (PR #245)
- **Phase A** ‚úÖ ‚Äî Token metrics + latency bug fix
- **Phase B** ‚úÖ ‚Äî Extended suite P13-P30 (18 prompts, 3 categories)
- **Phase C** ‚úÖ ‚Äî Single entrypoint consolidation (compare + model-compare modes)
- **Phase D** ‚úÖ (conditional) ‚Äî llama-server provider wired, health checks pass, full routing limited by CPU

### PR #245 CI
- Commit Messages ‚úÖ
- PR Attribution ‚úÖ
- AWS Amplify ‚ùå (unrelated to our work)
