# AI Experiment Tracks

This workspace collects AI constraint toolkits we can evaluate while prototyping agentic apps. Each toolkit folder contains a hand-off packet (`README`, `UNDERSTANDING`, `TASKS`, `NEXT_STEPS`, `HANDOFF`) so another agent can immediately continue the investigation.

## ğŸš€ Quick Start

1. **Understand the landscape**: Read [COMPARISON.md](./COMPARISON.md) for benefits/downsides
2. **Review criteria**: See [COMPARISON_CRITERIA.md](./COMPARISON_CRITERIA.md) for evaluation framework
3. **Implementation plan**: Check [IMPLEMENTATION_FRAMEWORK.md](./IMPLEMENTATION_FRAMEWORK.md) for the unified demo approach
4. **Task tracking**: See [IMPLEMENTATION_TASKS.md](./IMPLEMENTATION_TASKS.md) for all implementation tasks

## ğŸ¯ Comparison Approach

We're building the **same demo application** (Travel Planning Assistant) with each tool to enable fair comparison across:
- Implementation effort & developer experience
- Constraint effectiveness & validation
- Performance (latency, tokens, retries)
- Production readiness

## Current Shortlist

| Toolkit | Repo | Constraint Strategy | Category | Recommended Use Case |
| --- | --- | --- | --- | --- |
| Spec Kit | `github/spec-kit` | Spec-first workflow that compiles Markdown specs into JSON schemas and guard policies | Runtime Validation | Auditable policy compliance |
| Guardrails AI | `guardrails-ai/guardrails` | `RAIL` definition files plus validators, re-asking, and security filters | Runtime Validation | Customer-facing compliance |
| Microsoft Guidance | `microsoft/guidance` | Token-level control via templating, regex/JSON schema constraints | Token-Level | Multi-step orchestration |
| Outlines | `normal-computing/outlines` | Grammars and finite-state machines to guarantee outputs | Token-Level | Structured data generation |
| B-MAD Method | `bmad-code-org/BMAD-METHOD` | Development methodology framework with specialized agents | Workflow | AI-driven development lifecycle |
| **Beckett** | â“ Unknown | â“ TBD | â“ TBD | â“ Needs clarification |

If we identify more candidates, follow the same folder template and append them to the table above.

## ğŸ“ Folder Layout

```
ai_experiments/
â”œâ”€â”€ README.md                    # This file - overview
â”œâ”€â”€ COMPARISON.md                # Benefits/downsides comparison
â”œâ”€â”€ COMPARISON_CRITERIA.md       # Evaluation criteria
â”œâ”€â”€ IMPLEMENTATION_FRAMEWORK.md  # Unified demo approach
â”œâ”€â”€ IMPLEMENTATION_TASKS.md      # All implementation tasks
â”œâ”€â”€ common/                      # Shared test data & schemas
â”‚   â”œâ”€â”€ test_prompts.json       # Unified test prompts
â”‚   â””â”€â”€ expected_schemas.json   # Expected response schema
â”œâ”€â”€ benchmarks/                  # Cross-tool benchmarking
â”‚   â”œâ”€â”€ run_benchmarks.py       # Benchmark runner
â”‚   â””â”€â”€ results/                # Benchmark results
â””â”€â”€ <toolkit>/                   # Per-toolkit folders
    â”œâ”€â”€ README.md               # Overview + integration notes
    â”œâ”€â”€ UNDERSTANDING.md        # Research context & assumptions
    â”œâ”€â”€ TASKS.md                # Actionable backlog
    â”œâ”€â”€ NEXT_STEPS.md           # Prioritized near-term plan
    â”œâ”€â”€ HANDOFF.md              # Quick-start for the next agent
    â””â”€â”€ demo/                   # Implementation (when built)
```

Each document favors brevity and clear action items so agents can pick up work without re-reading the entire repository.

## ğŸ“Š Implementation Status

| Tool | Documentation | Demo Implementation |
|------|---------------|---------------------|
| Spec Kit | âœ… Complete | â¬œ Not Started |
| Guardrails AI | âœ… Complete | â¬œ Not Started |
| Microsoft Guidance | âœ… Complete | â¬œ Not Started |
| Outlines | âœ… Complete | â¬œ Not Started |
| B-MAD Method | âœ… Complete | â¬œ Not Started |
| Beckett | â“ Unknown | â“ Unknown |
