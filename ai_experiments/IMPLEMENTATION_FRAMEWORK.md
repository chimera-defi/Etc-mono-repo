# AI Guiding Tools - Implementation Framework

This document defines a unified approach to comparing AI constraint/guiding tools through practical implementation.

## ğŸ¯ Comparison Philosophy

To fairly compare these tools, we'll build **the same application** with each one, measuring:
1. **Implementation Effort** - Time and complexity to build
2. **Constraint Effectiveness** - How well it enforces rules
3. **Performance** - Latency, token usage, reliability
4. **Developer Experience** - Ergonomics, debugging, maintenance
5. **Production Readiness** - Scalability, observability, security

---

## ğŸ—ï¸ Unified Demo Application: "Smart Assistant API"

We'll build a **travel planning assistant** that must:
1. Accept natural language queries about trip planning
2. Return structured JSON responses (dates, destinations, budgets)
3. Enforce content policies (no illegal activities, PII handling)
4. Validate business rules (budget limits, date constraints)
5. Handle multi-turn conversations with context

### Why This Demo?
- **Structured outputs**: Tests JSON schema enforcement
- **Safety constraints**: Tests content filtering
- **Business rules**: Tests custom validation logic
- **Real-world relevance**: Representative of production use cases

---

## ğŸ“Š Tool Categories

### Category A: Runtime Validation (Post-Generation)
| Tool | Focus | Primary Use |
|------|-------|-------------|
| **Spec Kit** | Spec-first workflow | Auditable policy compliance |
| **Guardrails AI** | Safety + structure | Customer-facing compliance |

### Category B: Generation-Time Constraints (Token-Level)
| Tool | Focus | Primary Use |
|------|-------|-------------|
| **Microsoft Guidance** | Template orchestration | Multi-step workflows |
| **Outlines** | Grammar-constrained decoding | Guaranteed valid structures |

### Category C: Development Workflow (Process-Level)
| Tool | Focus | Primary Use |
|------|-------|-------------|
| **B-MAD Method** | AI-driven methodology | Full development lifecycle |

### Category D: To Be Researched
| Tool | Status | Notes |
|------|--------|-------|
| **Beckett** | â“ Unknown | Needs clarification - not found in documentation |
| **Others?** | â“ | Additional tools to compare? |

---

## ğŸ› ï¸ Implementation Plan Per Tool

### 1. Spec Kit Implementation
**Location:** `ai_experiments/spec_kit/demo/`

```
spec_kit/demo/
â”œâ”€â”€ specs/
â”‚   â””â”€â”€ travel-assistant.md      # Human-readable spec
â”œâ”€â”€ builds/
â”‚   â””â”€â”€ travel-assistant.json    # Compiled guards
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.ts                # Express endpoint
â”‚   â””â”€â”€ validator.ts             # Spec Kit integration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ compliance.test.ts       # Spec compliance tests
â””â”€â”€ package.json
```

**Tasks:**
| ID | Task | Priority | Estimated Effort |
|----|------|----------|------------------|
| SK-IMPL-01 | Create travel assistant spec in Markdown | High | 2 hours |
| SK-IMPL-02 | Compile spec to JSON guards | High | 30 min |
| SK-IMPL-03 | Build Express server with validation | High | 2 hours |
| SK-IMPL-04 | Implement retry logic on validation failure | Medium | 1 hour |
| SK-IMPL-05 | Add telemetry/logging | Medium | 1 hour |
| SK-IMPL-06 | Write compliance test suite | Medium | 2 hours |
| SK-IMPL-07 | Benchmark latency and token usage | Low | 1 hour |

---

### 2. Guardrails AI Implementation
**Location:** `ai_experiments/guardrails_ai/demo/`

```
guardrails_ai/demo/
â”œâ”€â”€ rails/
â”‚   â””â”€â”€ travel_assistant.rail    # RAIL spec file
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ budget_validator.py      # Custom budget rules
â”‚   â””â”€â”€ date_validator.py        # Date constraint validator
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # FastAPI endpoint
â”‚   â””â”€â”€ guard.py                 # Guardrails wrapper
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_compliance.py       # Pytest compliance tests
â””â”€â”€ requirements.txt
```

**Tasks:**
| ID | Task | Priority | Estimated Effort |
|----|------|----------|------------------|
| GR-IMPL-01 | Create travel assistant RAIL spec | High | 2 hours |
| GR-IMPL-02 | Implement custom budget validator | High | 1 hour |
| GR-IMPL-03 | Implement date constraint validator | High | 1 hour |
| GR-IMPL-04 | Build FastAPI server with Guard wrapper | High | 2 hours |
| GR-IMPL-05 | Configure re-ask strategies | Medium | 1 hour |
| GR-IMPL-06 | Add structured logging/telemetry | Medium | 1 hour |
| GR-IMPL-07 | Write pytest compliance suite | Medium | 2 hours |
| GR-IMPL-08 | Benchmark latency with 1/2/3 re-asks | Low | 1 hour |

---

### 3. Microsoft Guidance Implementation
**Location:** `ai_experiments/microsoft_guidance/demo/`

```
microsoft_guidance/demo/
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ travel_assistant.py      # Guidance template
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ trip_response.json       # JSON Schema
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # FastAPI endpoint
â”‚   â””â”€â”€ orchestrator.py          # Guidance program runner
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_templates.py        # Template tests
â””â”€â”€ requirements.txt
```

**Tasks:**
| ID | Task | Priority | Estimated Effort |
|----|------|----------|------------------|
| MG-IMPL-01 | Create Guidance template with JSON schema constraints | High | 2 hours |
| MG-IMPL-02 | Implement multi-step workflow (understand â†’ plan â†’ respond) | High | 2 hours |
| MG-IMPL-03 | Add regex constraints for dates/budgets | Medium | 1 hour |
| MG-IMPL-04 | Build FastAPI server with template execution | High | 1 hour |
| MG-IMPL-05 | Implement streaming token hooks | Medium | 1 hour |
| MG-IMPL-06 | Write template test suite | Medium | 1 hour |
| MG-IMPL-07 | Benchmark token-level enforcement overhead | Low | 1 hour |

---

### 4. Outlines Implementation
**Location:** `ai_experiments/outlines/demo/`

```
outlines/demo/
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ trip_response.py         # Pydantic models
â”‚   â””â”€â”€ trip_response.json       # JSON Schema
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # FastAPI endpoint
â”‚   â””â”€â”€ generator.py             # Outlines generator
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_generation.py       # Generation tests
â””â”€â”€ requirements.txt
```

**Tasks:**
| ID | Task | Priority | Estimated Effort |
|----|------|----------|------------------|
| OL-IMPL-01 | Define Pydantic models for trip response | High | 1 hour |
| OL-IMPL-02 | Create grammar-constrained generator | High | 1 hour |
| OL-IMPL-03 | Build FastAPI endpoint | High | 1 hour |
| OL-IMPL-04 | Add complex nested schema support | Medium | 1 hour |
| OL-IMPL-05 | Test with different sampling strategies | Medium | 1 hour |
| OL-IMPL-06 | Combine with post-hoc semantic validation | Medium | 2 hours |
| OL-IMPL-07 | Benchmark decoding speed with/without constraints | Low | 1 hour |

---

### 5. B-MAD Method Evaluation
**Location:** `ai_experiments/bmad/demo/`

```
bmad/demo/
â”œâ”€â”€ .bmad/                       # B-MAD project config
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ prd.md                   # Product requirements
â”‚   â”œâ”€â”€ architecture.md          # Architecture doc
â”‚   â””â”€â”€ stories/                 # User stories
â”œâ”€â”€ src/
â”‚   â””â”€â”€ travel_assistant/        # Generated app structure
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ custom_validator.md      # Custom workflow for constraints
â””â”€â”€ README.md
```

**Tasks:**
| ID | Task | Priority | Estimated Effort |
|----|------|----------|------------------|
| BM-IMPL-01 | Initialize B-MAD project with workflow-init | High | 30 min |
| BM-IMPL-02 | Use PM agent to create travel assistant PRD | High | 1 hour |
| BM-IMPL-03 | Use Architect agent for technical design | High | 1 hour |
| BM-IMPL-04 | Use Developer agent for implementation | High | 2 hours |
| BM-IMPL-05 | Create custom workflow for constraint validation | Medium | 2 hours |
| BM-IMPL-06 | Test B-MAD Builder for domain-specific agents | Medium | 2 hours |
| BM-IMPL-07 | Compare B-MAD output with manually built implementations | Low | 1 hour |

---

## ğŸ“ˆ Benchmark Criteria

### Quantitative Metrics
| Metric | Description | Target |
|--------|-------------|--------|
| **Latency (p50/p95/p99)** | Time from request to validated response | <500ms p95 |
| **Token Usage** | Tokens consumed per successful request | Minimize |
| **Validation Rate** | % of responses passing validation first try | >90% |
| **Retry Rate** | Average retries needed per request | <0.5 |
| **Error Rate** | % of requests that fail even after retries | <1% |

### Qualitative Metrics
| Metric | Description | Score 1-5 |
|--------|-------------|-----------|
| **Code Clarity** | How readable is the constraint code? | - |
| **Debugging Experience** | How easy to diagnose failures? | - |
| **Documentation Quality** | How helpful are the docs? | - |
| **Flexibility** | How easy to modify constraints? | - |
| **Testing Support** | How easy to test constraints? | - |

---

## ğŸ§ª Test Scenarios

Each implementation must handle these test cases:

### Happy Path
1. Valid trip request â†’ Structured JSON response
2. Multi-turn conversation â†’ Context preserved

### Constraint Enforcement
3. Budget exceeds limit â†’ Validation triggers, corrective action
4. Invalid date range â†’ Schema validation fails
5. Missing required field â†’ Re-prompt or error

### Safety Filters
6. Request for illegal activity â†’ Blocked
7. PII in response â†’ Filtered or redacted
8. Prompt injection attempt â†’ Defended

### Edge Cases
9. Ambiguous query â†’ Clarification requested
10. Very long context â†’ Graceful handling
11. Rate limiting â†’ Proper error response

---

## ğŸ“ Project Structure

```
ai_experiments/
â”œâ”€â”€ IMPLEMENTATION_FRAMEWORK.md    # This document
â”œâ”€â”€ COMPARISON.md                  # Benefits/downsides comparison
â”œâ”€â”€ COMPARISON_CRITERIA.md         # Evaluation criteria
â”œâ”€â”€ README.md                      # Overview
â”œâ”€â”€ benchmarks/                    # Cross-tool benchmark scripts
â”‚   â”œâ”€â”€ run_benchmarks.py
â”‚   â””â”€â”€ results/
â”œâ”€â”€ common/                        # Shared test data
â”‚   â”œâ”€â”€ test_prompts.json
â”‚   â””â”€â”€ expected_schemas.json
â”œâ”€â”€ spec_kit/
â”‚   â”œâ”€â”€ demo/                     # Implementation
â”‚   â””â”€â”€ *.md                      # Documentation
â”œâ”€â”€ guardrails_ai/
â”‚   â”œâ”€â”€ demo/                     # Implementation
â”‚   â””â”€â”€ *.md
â”œâ”€â”€ microsoft_guidance/
â”‚   â”œâ”€â”€ demo/                     # Implementation
â”‚   â””â”€â”€ *.md
â”œâ”€â”€ outlines/
â”‚   â”œâ”€â”€ demo/                     # Implementation
â”‚   â””â”€â”€ *.md
â””â”€â”€ bmad/
    â”œâ”€â”€ demo/                     # Implementation
    â””â”€â”€ *.md
```

---

## â±ï¸ Implementation Timeline

### Phase 1: Foundation (Week 1)
- [ ] Set up common test data and schemas
- [ ] Create benchmark infrastructure
- [ ] Implement Outlines demo (simplest integration)
- [ ] Implement Guardrails demo (most mature)

### Phase 2: Advanced (Week 2)
- [ ] Implement Guidance demo (token-level control)
- [ ] Implement Spec Kit demo (spec-first approach)
- [ ] Begin B-MAD evaluation

### Phase 3: Comparison (Week 3)
- [ ] Run full benchmark suite
- [ ] Document findings
- [ ] Create recommendation matrix
- [ ] Final comparison report

---

## â“ Open Questions (Need Clarification)

1. **What is "Beckett"?** 
   - Not found in current documentation
   - Is this another AI guiding tool to evaluate?
   - Please provide repository/documentation link

2. **Are there other tools to compare?**
   - LangChain output parsers?
   - Instructor (structured outputs for OpenAI)?
   - DSPy assertions?
   - LMQL (query language)?

3. **Language/Stack Preference**
   - Python-first (Guardrails, Guidance, Outlines)?
   - TypeScript-first (Spec Kit)?
   - Or build both for each tool where possible?

4. **LLM Provider**
   - Which provider to use for benchmarks? (OpenAI, Anthropic, Azure?)
   - Use same provider across all tools for fair comparison?
   - Or test provider flexibility as a criterion?

5. **Priority Order**
   - Which tools should we implement first?
   - Any specific tool more important for your use case?

6. **Deployment Target**
   - Server-side only?
   - Edge/serverless considerations?
   - On-device constraints relevant?

---

## ğŸ”„ Next Steps

Once clarifications are received:
1. Create common test data and schemas
2. Set up benchmark infrastructure
3. Implement demos in priority order
4. Run comparisons and document findings
