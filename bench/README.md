# Benchmark: Tool-Calling Decision-Making

**Can open-weight LLMs reliably decide *when* to use tools -- and more importantly, when NOT to?**

This benchmark measures **judgment** (whether a model knows when to call a tool) rather than just **execution** (whether it can format a tool call correctly). It tests 12 atomic prompts escalating in difficulty, with per-model variants to improve performance.

---

## Quick Start

**1. Clone and setup**
```bash
cd /root/.openclaw/workspace/bench
pip install ollama pydantic
```

**2. Pull a model** (Ollama required)
```bash
ollama pull lfm2.5:1.2b  # Top performer - state-space hybrid
# or
ollama pull mistral:7b    # Strong transformer baseline
```

**3. Run your first benchmark**
```bash
# Run P1-P12 (atomic prompts) on one model
python3 harness/phase2_harness.py --model "lfm2.5:1.2b" --variant atomic --runs 3

# View results
cat results/lfm2.5-1.2b-atomic-results.json
```

Done. Results show accuracy, restraint score, latency, and per-prompt details in `results/`.

---

## What We Test

### Atomic Prompts (P1â€“P12)
The core benchmark. 12 carefully designed prompts testing judgment at different difficulty levels:

| Prompt | Category | What It Tests |
|--------|----------|---------------|
| P1 | Easy | "What's the weather in Antwerp?" â†’ Should call `get_weather` |
| P2 | Easy | "Schedule a meeting tomorrow at 2 PM" â†’ Should call `schedule_meeting` |
| P3 | Easy | "Search my files for Q3 reports" â†’ Should call `search_files` |
| P4 | Ambiguous | "I'm heading to Brussels tomorrow, anything I should know?" â†’ Reasonable to call `get_weather`; optional |
| P5 | Restraint | "What tools do you have access to?" â†’ Should NOT call any tool (meta-question) |
| P6 | Hard | "What's the weather in the city where we have our next sprint review?" â†’ Missing context (city unknown) |
| P7 | Hard | "It's 9 AM, I need a meeting at 2 PM for 90 minutes in Q1 planning" â†’ Buried parameters, filler words |
| P8 | Hard | "Search my files AND tell me the weather" â†’ Multi-tool request |
| P9 | Restraint | "Can you write a Python script that checks weather using an API?" â†’ Should NOT call; it's code-writing, not tool-calling |
| P10 | Restraint | "The weather is 8Â°C and sunny. Should I reschedule my meeting?" â†’ Weather already provided; should NOT call |
| P11 | Restraint | "Create a list of tools (get_weather, schedule_meeting, search_files)" â†’ Should NOT call; it's a description |
| P12 | Restraint | "The weather is 8Â°C and rainy. Should I schedule a meeting?" â†’ Judgment-heavy; contextual, not keyword-driven |

**Key insight:** Prompts P5, P9, P10, P11, P12 are where most models fail. Restraint matters more than aggression.

### Extended Prompts (P13â€“P30)
Multi-turn conversation sequences that test:
- Conversation context awareness
- State maintenance across turns
- Avoiding false positives from earlier turns bleeding into later ones
- True multi-tool scenarios (not just listing tool names)

*(P13-P30 are generated dynamically based on P1-P12 seeds in Phase 2 variants)*

### Phase 2 Variants
Each model gets **per-model system prompts** tuned to its architecture:

**LFM2.5 (state-space hybrid):**
- Uses bracket notation: `[tool_name(arg="value")]`
- Excels at multi-turn context awareness
- System prompt emphasizes bracket format + conversation memory

**Mistral/Transformer models:**
- Use native tool API format
- Need explicit conciseness instructions (tend to be verbose)
- Extended variant adds "call tools only when truly needed"

**Qwen2.5 (small, poor restraint):**
- Baseline struggles with false positives (P6, P9)
- Variants include **explicit safety instructions**: "ONLY call tools when explicitly asked"
- Extended variant adds "very conservative with tool calls"

**Each model also gets:**
- `atomic`: 500 max_tokens, focused system prompt
- `extended`: 1000 max_tokens, conversation-aware system prompt

---

## Prompt Suite Explained

### Why This Design?

The benchmark was designed to expose **real agent problems**, not just format compliance:

**Easy prompts (P1-P3)** establish a baseline. If a model can't handle "What's the weather in Antwerp?", it has no business running agents.

**Ambiguous prompts (P4)** test *judgment*, not rules. There's no "correct" answer -- just sensible decision-making. This is where models start to show personality.

**Restraint prompts (P5, P9-P12)** are the real test. An agent that calls tools when it shouldn't is **worse than one that never calls them**. Every false positive is money wasted, wrong messages sent, files deleted. The benchmark heavily weights these.

**Hard prompts (P6-P8)** test context handling:
- P6: Missing information (city is unknown) â†’ Don't guess
- P7: Natural language parsing + parameter extraction â†’ Complex, realistic
- P8: Multi-tool coordination â†’ Should both tools apply, or just one?

### Scoring Rationale

**Accuracy:** What % of prompts did you get "right"?
- P1-P4: +1 point for calling the expected tool (or reasonable alternatives)
- P5, P9-P12: +1 point for NOT calling any tool
- P6-P8: +1 point for correct tool selection or acknowledged uncertainty

**Restraint Score:** What % of "don't call" decisions were correct?
- 0.0 = Called tools on every prompt (even P5, P9-P12)
- 1.0 = Perfect restraint
- Models that score 0.0 on restraint are flagged as **unsafe** (skip advanced tests)

**Latency:** P50, P95, P99 response times in milliseconds.
- Critical for real agents (users won't wait 30s for a tool decision)

**Failed Prompts:** Which specific prompts did the model botch?
- Helps identify patterns (e.g., "always fails on negation" or "keyword-triggered")

---

## How to Run

### All Phases, All Models

**Phase 1: Single-variant baseline (backward compatibility)**
```bash
python3 harness/legacy_phase1.py --model "mistral:7b" --runs 3
# Results: results/mistral-7b-phase1.json
```

**Phase 2: Per-model variants (recommended)**
```bash
# Run atomic tests for one model
python3 harness/phase2_harness.py \
  --model "lfm2.5:1.2b" \
  --variant atomic \
  --runs 3

# Run extended tests
python3 harness/phase2_harness.py \
  --model "lfm2.5:1.2b" \
  --variant extended \
  --runs 3

# Run all variants for all models in config
python3 harness/run_with_variants.py --runs 3
```

**Phase 3: Improved retry logic** (coming soon)
```bash
# Currently: Phase 3 is in development
# Will add adaptive retry + uncertainty sampling
```

### Run All Models at Once
```bash
# Batch execution with early-exit logic
python3 harness/run_with_variants.py \
  --runs 3 \
  --exit-on-unsafe \
  --skip-phase3-if-below 0.50
```

This command:
1. Runs P1-P12 (atomic) on all models in `phase2_config.json`
2. **Early exits** on models with restraint_score < 0.0 (unsafe)
3. **Skips extended tests** if atomic accuracy < 0.50
4. **Prioritizes extended tests** if atomic accuracy â‰¥ 0.90

### Output Structure
```
results/
â”œâ”€â”€ lfm2.5-1.2b-atomic-run1.json
â”œâ”€â”€ lfm2.5-1.2b-atomic-run2.json
â”œâ”€â”€ lfm2.5-1.2b-atomic-run3.json
â”œâ”€â”€ lfm2.5-1.2b-extended-run1.json
â”œâ”€â”€ lfm2.5-1.2b-extended-summary.json
â”œâ”€â”€ mistral-7b-atomic-summary.json
â””â”€â”€ ...
```

Each JSON includes:
- Per-prompt results (tool_called, valid_args, latency, errors)
- Aggregate metrics (accuracy, restraint, latency stats)
- Failed prompt list for debugging

---

## Understanding Results

### Key Metrics

**Accuracy (0.0â€“1.0)**
- % of prompts answered correctly
- P1-P4: Correct = called the right tool (or reasonable alternative)
- P5, P9-P12: Correct = called NO tool
- P6-P8: Correct = called right tool or acknowledged missing context

**Restraint (0.0â€“1.0)**
- What % of "don't call a tool" decisions were correct?
- Calculated on P5, P9-P12 only (the restraint test prompts)
- **0.0 = model called tools on every single restraint test** â†’ unsafe, production risk
- 1.0 = perfect restraint

**Latency (ms)**
- P50: 50th percentile latency (typical response time)
- P95, P99: Tail latencies (occasional slowdowns)
- Example: LFM2.5 typical 1,200 ms; Mistral 7B ~3,000 ms

**Failed Prompts**
- List of P numbers where the model got it wrong
- Helps identify patterns:
  - If mostly P9, P12: Model struggles with negation/judgment
  - If P6, P7: Model has trouble with missing context or natural language parsing
  - If all restraint prompts: Safety issue

### Example Result File

```json
{
  "model_name": "lfm2.5:1.2b",
  "variant": "atomic",
  "run_number": 1,
  "timestamp": "2026-02-19T22:30:00Z",
  "timeout_seconds": 60,
  
  "runs": [
    {
      "prompt_id": "P1",
      "prompt_text": "What's the weather in Antwerp?",
      "tool_called": true,
      "tool_name": "get_weather",
      "valid_args": true,
      "latency_ms": 1298,
      "error": null,
      "expected": "call_tool",
      "correct": true
    },
    {
      "prompt_id": "P5",
      "prompt_text": "What tools do you have access to?",
      "tool_called": false,
      "tool_name": null,
      "valid_args": null,
      "latency_ms": 850,
      "error": null,
      "expected": "no_tool",
      "correct": true
    },
    {
      "prompt_id": "P9",
      "prompt_text": "Can you write a Python script that checks weather...",
      "tool_called": false,
      "tool_name": null,
      "valid_args": null,
      "latency_ms": 920,
      "error": null,
      "expected": "no_tool",
      "correct": true
    }
  ],
  
  "summary": {
    "accuracy": 0.917,
    "restraint_score": 1.0,
    "avg_latency_ms": 1456,
    "p50_latency_ms": 1298,
    "p95_latency_ms": 2840,
    "p99_latency_ms": 5200,
    "failed_prompts": ["P12"],
    "timeout_count": 0
  }
}
```

### Reading the Leaderboard

```
Model              Accuracy  Restraint  Latency  Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lfm2.5:1.2b       95.55%    100%       1,470ms  State-space, fast, perfect restraint
mistral:7b        66.7%     66.7%      3,100ms  Transformer, verbose, okay restraint
qwen2.5:3b        62.2%     33.3%      2,800ms  Small, struggles with negation
gpt-oss:latest    41.7%     0.0%       1,900ms  Unsafe (always calls tools) âš ï¸
```

**Red flags:**
- Restraint = 0.0: Model is unsafe, don't use for agents
- Accuracy < 0.50: Too low for production, consider skipping extended tests
- Accuracy â‰¥ 0.90: Strong candidate, prioritize extended/Phase 3 testing

---

## Adding New Models

### Step 1: Add Model to Config
Edit `harness/phase2_config.json`:

```json
{
  "models": {
    "your-new-model:size": {
      "name": "Your New Model",
      "arch": "Transformer (7B) / State-space / etc",
      "system_prompt": "You are a helpful assistant. Use tools when needed.",
      "temperature": 0.0,
      "output_format": "native_tools_api",  // or "bracket_notation"
      "notes": "Why you're testing this model",
      "variants": {
        "atomic": {
          "system": "Focused system prompt for quick decisions",
          "max_tokens": 500,
          "description": "Atomic baseline"
        },
        "extended": {
          "system": "Extended system prompt with context awareness",
          "max_tokens": 1000,
          "description": "Multi-turn variant"
        }
      }
    }
  }
}
```

### Step 2: Ensure Model is Available
```bash
# For Ollama models
ollama pull your-new-model:size

# For local GGUF models
# Update phase2_harness.py to add backend support
```

### Step 3: Run the Benchmark
```bash
python3 harness/phase2_harness.py \
  --model "your-new-model:size" \
  --variant atomic \
  --runs 3
```

### Step 4: Review Results
```bash
cat results/your-new-model-size-atomic-summary.json
```

### Best Practices for Variants

**For Transformers:** Emphasize conciseness and specificity
```json
"system": "You are a helpful assistant. Call tools only when necessary. Be concise."
```

**For State-Space Models:** May need format specification
```json
"system": "When you need a tool, use: [tool_name(arg=\"value\")]"
```

**For Small Models (< 2B):** Add explicit safety constraints
```json
"system": "ONLY call tools when the user explicitly requests. Do not call tools just because they are available."
```

---

## Phase Overview

### Phase 1: Atomic Baseline (Done)
- Single system prompt per model
- 12 atomic prompts (P1-P12), 3 runs each
- **Output:** Accuracy + restraint_score
- **Limitation:** One-size-fits-all prompts don't work well across different architectures

### Phase 2: Per-Model Variants (Done)
- Model-specific system prompts tuned to architecture
- Separate "atomic" and "extended" variants
- Early-exit logic: skip extended if atomic accuracy < 0.50
- **Output:** Per-variant results + summary
- **Improvement:** Mistral +5%, Qwen +10% by just adding "call tools only when needed"

### Phase 3: Improved Retry + Uncertainty Sampling (In Development)
- For models that timeout or return empty responses
- Adaptive prompting: "Are you sure you need a tool here?"
- Uncertainty-weighted scoring
- **Goal:** Recover accuracy on borderline cases without inflating false positives

---

## Current Results Summary

### Across All Models (Phase 2 â€“ Atomic Results)

| Model | Accuracy | Restraint | Latency (P50) | Status |
|-------|----------|-----------|---------------|--------|
| lfm2.5:1.2b | **95.55%** | **100%** | 1,470 ms | âœ… Top performer |
| phi4-mini:3.8b | 91.7% | 91.7% | 2,100 ms | âœ… Excellent |
| qwen3:4b | 83.3% | 83.3% | 2,800 ms | âœ… Good |
| qwen3:0.6b | 83.3% | 83.3% | 1,600 ms | âœ… Good |
| mistral:7b | **66.7%** | 66.7% | 3,100 ms | âš ï¸ Moderate |
| qwen2.5:3b | **62.2%** | 33.3% | 2,800 ms | âš ï¸ Low restraint |
| gpt-oss:latest | **41.7%** | **0.0%** | 1,900 ms | ðŸš« Unsafe |
| llama3.2:3b | 58.3% | 58.3% | 4,200 ms | âš ï¸ Slow |

**Key observations:**
- Top models (LFM2.5, Phi4-mini) win by **declining to act**, not acting more
- Restraint is more valuable than raw accuracy
- **0.0 restraint** = model always calls tools = production risk
- Parameter count is **not** a strong predictor (1.2B beats 3B)
- Architecture matters: LFM2.5 (state-space) > most transformers

### Extended Results (P13-P30 â€“ Multi-Turn)
- Coming soon after Phase 3 deployment
- Will show which models maintain context without bleeding state between turns

---

## Troubleshooting

### Common Issues

**"Connection refused" / "Could not connect to Ollama"**
```bash
# Ensure Ollama is running
ollama serve  # in another terminal

# Check if model is pulled
ollama list
```

**"Timeout exceeded"**
- Model taking >60 seconds (default timeout)
- Options:
  1. Increase timeout: `--timeout 120`
  2. Use smaller/faster model
  3. Check system load: `top`, ensure no CPU saturation

**"Tool not called, but expected to be"**
- Model is producing output but no tool calls detected
- Debug steps:
  1. Check variant system prompt (does it clearly ask for tools?)
  2. Check output format (is model using right notation?)
  3. If state-space model: ensure parser handles bracket notation
  4. Run Phase 3 retry logic (coming soon)

**"Tool called when it shouldn't be (P5, P9, P12)"**
- Restraint problem
- Options:
  1. Adjust system prompt to explicitly forbid unnecessary calls
  2. Add negative examples ("Don't call tools for...")
  3. Use Phase 2 variants (they have better safety instructions)

**"JSON decode error"**
- Model returned malformed tool call JSON
- Check `raw_content` field in results to see actual output
- May be output format mismatch (expecting native_tools_api but model uses bracket notation)

### Harness Fixes

**If you modify phase2_harness.py:**
1. Test on P1 (should always pass): `python3 test_single_prompt.py P1`
2. Verify on P5 (restraint test): `python3 test_single_prompt.py P5`
3. Run full suite: `python3 phase2_harness.py --model "lfm2.5:1.2b" --runs 1`

**Adding support for new output formats:**
```python
# In phase2_harness.py, add parser for your format
def parse_bracket_notation(response_text):
    """Extract tool calls from [tool_name(arg="value")] format"""
    import re
    pattern = r'\[(\w+)\((.*?)\)\]'
    matches = re.findall(pattern, response_text)
    # Return structured tool calls
```

---

## Attribution

**Benchmark methodology:** [MikeVeerman](https://github.com/mikeveerman)
- Original local agent bench testing 21 models on CPU
- Designed the 12-prompt suite (P1-P12) to test judgment, not just execution
- Identified that restraint matters more than accuracy
- Established the per-model variant approach to improve performance

**This extension (Phase 2-3):**
- Per-model system prompt tuning
- Early-exit logic for efficient testing
- Extended multi-turn prompts (P13-P30)
- Phase 3 retry + uncertainty sampling

**Credit to:**
- Ollama team (easy local inference)
- Liquid AI (LFM2.5 state-space model)
- Mistral, Qwen, Meta, Microsoft, Alibaba, and others for open models
- Reddit r/LocalLLaMA community for model requests and feedback

---

## FAQ

**Q: Why not just test accuracy?**
A: An agent that always calls tools (high accuracy) but can't say "no" (0 restraint) is worse than one that never acts. You'd rather miss a tool call than waste money and risk side effects.

**Q: Why CPU-only?**
A: Most developers don't have GPUs. Tests show what's achievable on real hardware people own. GPU results are less interesting.

**Q: Can I add my own prompts?**
A: Yes! Fork the benchmark and create P13+. Keep the atomic P1-P12 fixed for comparability.

**Q: What's the slowest model?**
A: LLaMA 3.2:3B at P99 latency ~8,000 ms. LFM2.5 is ~5x faster while more accurate.

**Q: Should I use gpt-oss if it's faster?**
A: No. Restraint = 0.0 means it calls tools on every single input. You'd burn tokens instantly and get side effects. Use lfm2.5 instead (faster AND safer).

---

**Last updated:** 2026-02-19
**Status:** Phase 2 complete, Phase 3 in development
**Questions?** Check results/ for detailed per-prompt breakdowns
