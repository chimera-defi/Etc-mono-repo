# Benchmark Harness Architecture V2: Self-Optimizing Core

**Focus:** The recursive improvement loop IS the architecture, not a feature of it.

**Date:** 2026-02-23  
**Status:** Redesign before implementation

---

## Core Concept: The Optimization Loop

The system is fundamentally a **feedback loop that improves itself automatically**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SELF-OPTIMIZING LOOP                         â”‚
â”‚                    (This IS the architecture)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  INPUT: Baseline Config                                                  â”‚
â”‚  (Best performing spec from prior cycle, or initial guess)               â”‚
â”‚                                                                            â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 1: GENERATE IMPROVEMENT CANDIDATES                           â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ From baseline, create N variations:                                â”‚ â”‚
â”‚  â”‚  â€¢ Different variant (native_api â†’ atomic)                         â”‚ â”‚
â”‚  â”‚  â€¢ Different timeout (60s â†’ 120s)                                  â”‚ â”‚
â”‚  â”‚  â€¢ Different retry count (r1 â†’ r2)                                 â”‚ â”‚
â”‚  â”‚  â€¢ Different isolation mode (i0 â†’ i1)                              â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: [Baseline, Candidate-1, Candidate-2, ...]                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 2: EVALUATE ALL SPECS (Multiple Runs)                        â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ For each spec (baseline + candidates):                             â”‚ â”‚
â”‚  â”‚  â€¢ Run 7 times on same prompts                                     â”‚ â”‚
â”‚  â”‚  â€¢ Collect: accuracy, variance, latency, restraint                 â”‚ â”‚
â”‚  â”‚  â€¢ Analyze: pattern detection (cold-start vs capability)           â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: {baseline: {...}, candidates: [...]}, each with metrics   â”‚ â”‚
â”‚  â”‚ Key insight: Variance analysis reveals problem TYPE                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 3: APPLY DECISION GATES                                      â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ For each candidate, check gates:                                   â”‚ â”‚
â”‚  â”‚  â€¢ Gate 1: Min samples >= 3? (enough data?)                        â”‚ â”‚
â”‚  â”‚  â€¢ Gate 2: Variance <= 0.0025? (stable?)                           â”‚ â”‚
â”‚  â”‚  â€¢ Gate 3: No regression? (better or same as baseline?)            â”‚ â”‚
â”‚  â”‚  â€¢ Gate 4: Restraint >= 0.80? (model constrained?)                 â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Result per candidate: PROMOTE | HOLD | REJECT                     â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: Decision matrix + reasoning                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 4: AUTO-DIAGNOSE & RECOMMEND                                 â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Analyze patterns:                                                  â”‚ â”‚
â”‚  â”‚  â€¢ Cold-start pattern [F,F,P,P,P,P,P]? â†’ Recommend warm-up       â”‚ â”‚
â”‚  â”‚  â€¢ Zero variance 0%? â†’ Recommend routing to better model          â”‚ â”‚
â”‚  â”‚  â€¢ High variance? â†’ Recommend more retries                         â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: Auto-generated recommendations                             â”‚ â”‚
â”‚  â”‚ Key: NO human tuning needed                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 5: PICK WINNER & LEARN                                       â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ If any candidate PROMOTED:                                         â”‚ â”‚
â”‚  â”‚  â€¢ New baseline = best promoting candidate                         â”‚ â”‚
â”‚  â”‚  â€¢ Record: Why it won (metrics + gates)                            â”‚ â”‚
â”‚  â”‚  â€¢ Store: In meta_loop_history.json                                â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Key insight: Baseline improves monotonically (if learning works)   â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: {promoted_baseline, reason, metrics}                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PHASE 6: ENFORCE RECOMMENDATIONS                                   â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Apply auto-diagnosed recommendations:                              â”‚ â”‚
â”‚  â”‚  â€¢ Enable warm-up if cold-start detected                           â”‚ â”‚
â”‚  â”‚  â€¢ Route to fallback model if capability limit detected            â”‚ â”‚
â”‚  â”‚  â€¢ Adjust config if high variance detected                         â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Store all decisions:                                               â”‚ â”‚
â”‚  â”‚  â€¢ routing_config.json (enforcement rules)                         â”‚ â”‚
â”‚  â”‚  â€¢ routing_decisions.log (audit trail)                             â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚ Output: System auto-applies learnings                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–¼                                                                  â”‚
â”‚  OUTPUT: Improved Baseline (for next cycle)                              â”‚
â”‚  Better OR same, never worse (gates prevent regression)                  â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€ IF improved: Use as baseline for Cycle N+1                           â”‚
â”‚  â””â”€ IF no improvement: Try different candidates next time                â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LOOP CLOSES: New Baseline â†’ Phase 1 (repeat)                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY: Each cycle learns something. System improves by doing, not by tuning.
```

---

## What Makes It "Self-Optimizing"

**Traditional benchmarking (manual):**
```
1. Run benchmark
2. Human reads results
3. Human thinks "what to try next"
4. Human modifies config
5. Run again
6. Repeat
```

**Self-optimizing (automatic):**
```
1. Run benchmark (multiple specs)
2. System analyzes results
3. System detects patterns
4. System recommends improvements
5. System applies recommendations
6. System picks winner automatically
7. Winner becomes baseline
8. Repeat (baseline improves each cycle)
```

**The magic:** Steps 2-7 are automatic. No human in the loop.

---

## The Three Layers (Supporting the Loop)

The loop needs infrastructure. This is what layers do:

### Layer 1: EXECUTION (Runs the experiments)
```
run_benchmark.py
  â”œâ”€ Invoke model on prompts
  â”œâ”€ Measure: accuracy, latency, restraint
  â”œâ”€ Handle: retries, timeouts, failures
  â””â”€ Output: Single run result {accuracy: 0.667, ...}
```

**Why this layer:** Loop needs raw data. Runner generates it.

---

### Layer 2: EVALUATION (Analyzes the data)
```
benchmark_supervisor.py
  â”œâ”€ Run spec 7 times
  â”œâ”€ Aggregate: median, variance
  â”œâ”€ Detect patterns: [F,F,P,P,P,P,P] vs [F,F,F,F,F,F,F]
  â””â”€ Output: Analyzed result {accuracy: 0.667, variance: 0.0117, pattern: [F,F,P...]}

self_optimizing_policy.py
  â”œâ”€ Apply gate 1: min samples?
  â”œâ”€ Apply gate 2: variance?
  â”œâ”€ Apply gate 3: regression?
  â”œâ”€ Apply gate 4: restraint?
  â””â”€ Output: Decision {promote | hold | reject, reason}
```

**Why this layer:** Loop needs to classify results. Gates do it.

---

### Layer 3: ORCHESTRATION (Controls the loop)
```
meta_harness_loop.py
  â”œâ”€ Run cycle N
  â”œâ”€ Collect all results
  â”œâ”€ Apply policy decisions
  â”œâ”€ Pick winner
  â”œâ”€ Store in history
  â””â”€ Prepare for cycle N+1

harness_feedback_loop.py
  â”œâ”€ Analyze patterns
  â”œâ”€ Auto-diagnose issues
  â”œâ”€ Generate recommendations
  â””â”€ Output: harness_feedback.json

routing_enforcer.py
  â”œâ”€ Read recommendations
  â”œâ”€ Generate routing_config.json
  â”œâ”€ Log to routing_decisions.log
  â””â”€ Make recommendations actionable
```

**Why this layer:** Loop needs to be orchestrated. Orchestrator does it.

---

## Data Flow for Self-Optimization

```
â”Œâ”€ Baseline Spec
â”‚  {model: lfm, phase: atomic, variant: native_api, t: 60, r: 1}
â”‚  [Cycle 1 winner, or initial guess]
â”‚
â”œâ”€ Generate Candidates
â”‚  Candidate-1: variant: atomic   (change 1 thing)
â”‚  Candidate-2: timeout: 120s     (change 1 thing)
â”‚  Candidate-3: retries: 2        (change 1 thing)
â”‚
â”œâ”€ Evaluate
â”‚  Run each 7 times on same prompts
â”‚  Baseline: [0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667] â†’ median 0.667, var 0.0
â”‚  Cand-1:   [0.833, 0.833, 0.833, 0.833, 0.833, 0.833, 0.833] â†’ median 0.833, var 0.0 âœ“
â”‚  Cand-2:   [0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667] â†’ median 0.667, var 0.0
â”‚  Cand-3:   [0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667] â†’ median 0.667, var 0.0
â”‚
â”œâ”€ Apply Gates
â”‚  Baseline: No gates (is baseline)
â”‚  Cand-1: Gate1âœ“ Gate2âœ“ Gate3âœ“ Gate4âœ“ â†’ PROMOTE (median 0.833 > baseline 0.667)
â”‚  Cand-2: Gate1âœ“ Gate2âœ“ Gate3âœ— Gate4âœ“ â†’ REJECT (no improvement)
â”‚  Cand-3: Gate1âœ“ Gate2âœ“ Gate3âœ— Gate4âœ“ â†’ REJECT (no improvement)
â”‚
â”œâ”€ Auto-Diagnose
â”‚  Baseline pattern: [P,P,P,P,P,P,P] at 0.667
â”‚    Diagnosis: Clean runs, no infrastructure issue, but suboptimal
â”‚    Recommendation: Try different variant (done - Cand-1 worked!)
â”‚
â”‚  Extended phase for LFM: [F,F,F,F,F,F,F] at 0.0
â”‚    Diagnosis: Zero variance = model capability limit
â”‚    Recommendation: Route to claude-haiku for extended
â”‚
â”œâ”€ Store Results
â”‚  Winner: Candidate-1 (variant: atomic)
â”‚  Baseline accuracy improved: 0.667 â†’ 0.833 (+16.6%)
â”‚  Record in meta_loop_history.json
â”‚  {cycle: 1, baseline_was: native_api, winner: atomic, improvement: 0.166}
â”‚
â”œâ”€ Cycle N+1 Uses New Baseline
â”‚  NEW baseline: atomic variant (0.833)
â”‚  NEW candidates: timeout variants, retry variants, etc.
â”‚  Can we beat 0.833? â†’ Find out in next cycle
â”‚
â””â”€ LEARNING: System now knows variant matters, warmup helps, extended needs fallback
   This knowledge persists in recommendations & routing rules.
```

---

## The Improvement Curve (What We Expect)

```
                    BASELINE IMPROVEMENT OVER CYCLES
                    
Accuracy %
â”‚
100 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† Ceiling (model capability limit)
 95 â”‚                          â”‚
 90 â”‚                        â”Œâ”€â”˜
 85 â”‚                      â”Œâ”€â”˜
 80 â”‚                    â”Œâ”€â”˜â”€ Cycle 2: New candidates vs atomic baseline
 75 â”‚                  â”Œâ”€â”˜    (try timeout, retries)
 70 â”‚              â”Œâ”€â”˜â”€ Cycle 1: atomic vs native_api baseline
 65 â”‚            â”Œâ”€â”˜    (improvement found: +16.6%)
 60 â”‚          â”€â”€â”˜      Initial: native_api baseline (66.7%)
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â†’ Cycle
      Cycle 1  2  3  4  5  6  7

KEY:
â€¢ Each step up = one cycle found improvement
â€¢ Flat line = no improvement found (ceiling reached or exploration exhausted)
â€¢ Line never goes down (gates prevent regression)
â€¢ At some point: plateau (model limit) or need new model tier
```

---

## What Needs to Change for True Self-Optimization

**Currently working:**
- âœ… Variance detection
- âœ… Policy gates
- âœ… Recommendation generation
- âœ… History tracking

**Missing: Enforcement Loop Closure**
Currently:
1. System recommends: "Enable warm-up"
2. Recommendations sit in harness_feedback.json
3. Humans read them manually
4. Humans apply them manually
5. System never automatically acts

**What's needed:**
1. System recommends: "Enable warm-up"
2. System checks: Is warm-up enabled?
3. System auto-enables if not
4. System re-runs with warm-up
5. System verifies improvement
6. System updates config permanently

**Missing piece: routing_enforcer.py + config management**

---

## Three Components of True Self-Optimization

### Component 1: Variance-Driven Diagnosis
```
Input: Raw metrics from 7 runs
Process: Detect pattern [F,F,P,P,P,P,P] or [F,F,F,F,F,F,F]
Output: Diagnosis (cold-start vs capability)
Status: âœ… WORKING (benchmark_supervisor.py)
```

### Component 2: Automatic Recommendations
```
Input: Variance diagnosis
Process: "Cold-start? â†’ try warm-up" "0%? â†’ route to Haiku"
Output: recommendations in harness_feedback.json
Status: âœ… WORKING (harness_feedback_loop.py)
```

### Component 3: Enforcement & Auto-Learning
```
Input: Recommendations
Process: Apply to config, re-run, verify improvement, store decision
Output: Updated routing rules, improved baseline
Status: ğŸ”´ MISSING (routing_enforcer.py exists but not auto-applying)
```

**Missing piece #3 is what makes it truly self-optimizing.**

---

## The Plan (Fixed)

Instead of just implementing features, we need to close the loop:

### Phase 0: Architecture Completion (Today - 1 hour)
- [ ] Complete routing_enforcer.py (read recommendations â†’ apply config)
- [ ] Create config management (routing_config.json is source of truth)
- [ ] Create auto-apply mechanism (system reads config on startup)
- [ ] Design feedback flow: Diagnosis â†’ Recommendation â†’ Enforcement â†’ New Baseline

### Phase 1: Implementation (Tomorrow - 3 hours)
- [ ] Modify run_benchmark.py to read routing_config.json
- [ ] Modify meta_harness_loop.py to call routing_enforcer
- [ ] Create auto-apply tests
- [ ] Implement lock fix (PID-based)

### Phase 2: Validation (Day 3 - 2 hours)
- [ ] Does warm-up recommendation auto-apply?
- [ ] Does extended fallback auto-enforce?
- [ ] Are decisions traceable?

### Phase 3: Full Loop Test (Day 4 - 1 hour)
- [ ] Run Cycle 1: Baseline vs variants
- [ ] System recommends improvement
- [ ] System auto-applies improvement
- [ ] Run Cycle 2: Is new baseline better?
- [ ] Verify: System learned and improved

---

## True Self-Optimization Checklist

âœ… = System does it automatically (no human intervention)

- [ ] âœ… Run multiple specs
- [ ] âœ… Detect variance patterns
- [ ] âœ… Classify patterns (cold-start vs capability)
- [ ] âœ… Generate recommendations
- [ ] âœ… **Auto-apply recommendations** â† KEY MISSING PIECE
- [ ] âœ… Measure improvement
- [ ] âœ… Update baseline
- [ ] âœ… Store decisions with reasoning
- [ ] âœ… Next cycle uses improved baseline
- [ ] âœ… Repeat until ceiling

**Without step 5 (auto-apply), it's not self-optimizingâ€”it's just reporting.**

---

## Success Definition

The system is truly self-optimizing when:

1. **Autonomous:** Cycles run without human intervention
2. **Learning:** Each cycle improves (or explains why not)
3. **Auditable:** Every decision traceable to data
4. **Closed-loop:** Recommendations automatically become reality
5. **Improving:** Baseline better than before (monotonic improvement)

---

## Architecture Summary (Redesigned)

```
THE SELF-OPTIMIZING LOOP

Phase 1 (Generate)  â†’  Phase 2 (Evaluate)  â†’  Phase 3 (Decide)
                              â†“
                    Phase 6 (Enforce)  â†  Phase 4 (Diagnose)
                              â†“
                         Phase 5 (Learn)
                              â†“
                    [New Baseline for Cycle N+1]
                              â†“
                         [LOOP REPEATS]

Layers support the loop:
- Layer 1 (Runner): Generate data for evaluation
- Layer 2 (Evaluator): Analyze data for decisions
- Layer 3 (Orchestrator): Control the loop + auto-apply learnings
```

---

## Next Step

Before any code: **Do we agree this is what self-optimization should be?**

If yes:
1. Finalize Phase 0 (architecture completion)
2. Then implement Phase 1-3 in sequence
3. Verify loop closure before considering complete

If no:
- What's missing from this model?
- What should true self-optimization include?

---

**Current Status:** ğŸŸ¡ Architecture redesigned, awaiting approval  
**Next Action:** Review + feedback on loop design  
**Then:** Implement with loop closure in mind (not just features)
