{
  "models": {
    "lfm2.5-thinking:1.2b": {
      "name": "LFM2.5-1.2B",
      "arch": "State-space hybrid",
      "system_prompt": "You are a helpful assistant with access to tools. When you need to call a tool, use bracket notation: [tool_name(arg=\"value\")].",
      "temperature": 0.0,
      "output_format": "bracket_notation",
      "notes": "Prefers bracket output. State-space models handle multi-turn well.",
      "variants": {
        "atomic": {
          "system": "When you need a tool, use: [tool_name(arg=\"value\")]",
          "max_tokens": 500,
          "description": "Original bracket format for atomic prompts"
        },
        "extended": {
          "system": "You are a helpful assistant. Remember conversation context. When tools are needed, use: [tool_name(arg=\"value\")]",
          "max_tokens": 1000,
          "description": "Extended with context awareness for multi-turn"
        }
      }
    },
    "mistral:7b": {
      "name": "Mistral 7B",
      "arch": "Transformer (7B)",
      "system_prompt": "You are a helpful assistant. Call tools when needed to fulfill user requests.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Transformer. Sometimes verbose. Add explicit instruction to be concise.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools when the user asks. Be concise.",
          "max_tokens": 500,
          "description": "Baseline with conciseness instruction"
        },
        "extended": {
          "system": "You are a helpful assistant. Pay attention to conversation context. Maintain state across turns. Call tools only when truly needed. Be concise.",
          "max_tokens": 1000,
          "description": "Extended with explicit state awareness + restraint reminder"
        }
      }
    },
    "gpt-oss:latest": {
      "name": "GPT-OSS",
      "arch": "Unknown (13GB, likely transformer)",
      "system_prompt": "You are a helpful assistant with tool access.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Excellent on atomic validation (87.5% before timeout). Fast judgment.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Use tools to answer user questions accurately.",
          "max_tokens": 500,
          "description": "Standard variant"
        },
        "extended": {
          "system": "You are a helpful assistant. Remember conversation context. Use tools appropriately based on what the user actually asks for, not contextual hints.",
          "max_tokens": 1000,
          "description": "Extended with context + explicit instruction to ignore contextual hints"
        }
      }
    },
    "qwen2.5:3b": {
      "name": "Qwen2.5 3B",
      "arch": "Transformer (3B)",
      "system_prompt": "You are a helpful assistant. Use tools to answer user questions.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Baseline 3B. Poor restraint (false positives P6, P9). Needs explicit safety instructions.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. ONLY call tools when the user explicitly asks. Do not call tools just because they are available. Be strict: if unsure, do not call.",
          "max_tokens": 500,
          "description": "Safety-focused: explicit restraint instructions"
        },
        "extended": {
          "system": "You are a helpful assistant. ONLY call tools when the user explicitly requests them. Ignore contextual hints. Maintain conversation history. Be very conservative with tool calls.",
          "max_tokens": 1000,
          "description": "Extended + strict safety"
        }
      }
    },
    "qwen2.5:14b": {
      "name": "Qwen2.5 14B",
      "arch": "Transformer (14B)",
      "system_prompt": "You are a helpful assistant. Use tools to answer user questions.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "14B variant. Larger model - should have better reasoning and restraint.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. ONLY call tools when the user explicitly asks. Do not call tools just because they are available. Be strict: if unsure, do not call.",
          "max_tokens": 500,
          "description": "Safety-focused: explicit restraint instructions"
        },
        "extended": {
          "system": "You are a helpful assistant. ONLY call tools when the user explicitly requests them. Ignore contextual hints. Maintain conversation history. Be very conservative with tool calls.",
          "max_tokens": 1000,
          "description": "Extended + strict safety"
        }
      }
    },
    "ministral-3:latest": {
      "name": "Ministral 3B",
      "arch": "Transformer (3B, Mistral optimized)",
      "system_prompt": "You are a helpful assistant.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Smaller variant. Unknown performance. Test as candidate.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools only when needed.",
          "max_tokens": 500,
          "description": "Baseline for new model"
        },
        "extended": {
          "system": "You are a helpful assistant. Track conversation context. Call tools sparingly.",
          "max_tokens": 1000,
          "description": "Extended variant"
        }
      }
    },
    "minimax/MiniMax-M2.5": {
      "name": "Minimax M2.5",
      "arch": "MoE (likely 456B params)",
      "provider": "online",
      "api": "minimax",
      "system_prompt": "You are a helpful assistant with access to tools.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Online model - 200k context window, excellent reasoning. Requires MINIMAX_API_KEY.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools when the user explicitly asks.",
          "max_tokens": 500,
          "description": "Fast variant for simple prompts"
        },
        "extended": {
          "system": "You are a helpful assistant. Maintain conversation context across turns. Call tools only when explicitly requested.",
          "max_tokens": 1000,
          "description": "Extended with context awareness"
        }
      }
    },
    "anthropic/claude-sonnet-4": {
      "name": "Claude Sonnet 4",
      "arch": "Transformer (unknown size)",
      "provider": "online",
      "api": "claude",
      "system_prompt": "You are a helpful assistant with access to tools.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Online model - Anthropic's latest. Requires ANTHROPIC_API_KEY.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools when the user explicitly asks.",
          "max_tokens": 500,
          "description": "Fast variant for atomic tasks"
        },
        "extended": {
          "system": "You are a helpful assistant. Maintain conversation context. Call tools only when explicitly requested.",
          "max_tokens": 1000,
          "description": "Extended with multi-turn context"
        }
      }
    },
    "anthropic/claude-haiku": {
      "name": "Claude Haiku",
      "arch": "Transformer (smaller)",
      "provider": "online",
      "api": "claude_haiku",
      "system_prompt": "You are a helpful assistant with access to tools.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "Online model - Fast and efficient. Requires ANTHROPIC_API_KEY.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools when asked.",
          "max_tokens": 500,
          "description": "Fast atomic variant"
        },
        "extended": {
          "system": "You are a helpful assistant. Keep context across messages. Call tools only when needed.",
          "max_tokens": 1000,
          "description": "Extended context variant"
        }
      }
    },
    "glm-4.7-flash:latest": {
      "name": "GLM-4.7-Flash",
      "arch": "GLM-4 (19GB)",
      "system_prompt": "You are a helpful assistant with access to tools.",
      "temperature": 0.0,
      "output_format": "native_tools_api",
      "notes": "GLM model from Zhipu AI. Fast inference.",
      "variants": {
        "atomic": {
          "system": "You are a helpful assistant. Call tools when the user asks. Be concise.",
          "max_tokens": 500,
          "description": "Baseline with conciseness instruction"
        },
        "extended": {
          "system": "You are a helpful assistant. Pay attention to conversation context. Maintain state across turns. Call tools only when truly needed. Be concise.",
          "max_tokens": 1000,
          "description": "Extended with explicit state awareness + restraint reminder"
        }
      }
    }
  },
  "online_models": {
    "minimax": {
      "api_base": "https://api.minimax.io/v1",
      "model": "MiniMax-M2.5",
      "env_key": "MINIMAX_API_KEY",
      "timeout": 120,
      "max_retries": 3,
      "rate_limit_rpm": 60,
      "context_window": 200000,
      "supports_tools": true
    },
    "claude": {
      "api_base": "https://api.anthropic.com/v1",
      "model": "claude-sonnet-4-20250514",
      "env_key": "ANTHROPIC_API_KEY",
      "timeout": 120,
      "max_retries": 3,
      "rate_limit_rpm": 50,
      "context_window": 200000,
      "supports_tools": true
    },
    "claude_haiku": {
      "api_base": "https://api.anthropic.com/v1",
      "model": "claude-3-haiku-20240307",
      "env_key": "ANTHROPIC_API_KEY",
      "timeout": 60,
      "max_retries": 3,
      "rate_limit_rpm": 50,
      "context_window": 200000,
      "supports_tools": true
    }
  },
  "fallback_chains": {
    "online_primary": [
      "minimax/MiniMax-M2.5",
      "anthropic/claude-haiku",
      "anthropic/claude-sonnet-4"
    ],
    "online_to_local": [
      "minimax/MiniMax-M2.5",
      "anthropic/claude-haiku",
      "lfm2.5-thinking:1.2b",
      "mistral:7b"
    ],
    "local_only": [
      "lfm2.5-thinking:1.2b",
      "mistral:7b"
    ]
  },
  "early_exit": {
    "atomic_fail_threshold": 0.5,
    "restraint_fail_threshold": 0.0,
    "rules": [
      {
        "id": "high_failure",
        "condition": "accuracy < 0.50 on P1-P12",
        "action": "SKIP extended tests",
        "reason": "Too low to warrant P13-P30 investment"
      },
      {
        "id": "zero_restraint",
        "condition": "restraint_score == 0.0",
        "action": "FLAG UNSAFE, skip advanced tests",
        "reason": "Model calls tools when it shouldn't - production risk"
      },
      {
        "id": "excellent_atomic",
        "condition": "accuracy >= 0.90 on P1-P12",
        "action": "PRIORITIZE extended tests",
        "reason": "Strong candidate for multi-turn evaluation"
      }
    ]
  },
  "timeout_seconds": 60
}