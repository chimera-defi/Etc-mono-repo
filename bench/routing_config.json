{
  "version": "1.3",
  "generated_at": "2026-02-23T20:39:00.000000",
  "strategy": {
    "tier_1": "online_models",
    "tier_2": "local_models",
    "prompt_complexity_routing": "enabled",
    "note": "Online models (Minimax, Claude, Codex) first. LFM as fallback for offline/low-cost scenarios. Prompt complexity routing enabled."
  },
  "online_apis": {
    "minimax": {
      "api_base": "https://api.minimax.io/v1",
      "model": "MiniMax-M2.5",
      "env_key": "MINIMAX_API_KEY",
      "timeout": 120,
      "max_retries": 3,
      "retry_delay": 1.0,
      "rate_limit_rpm": 60,
      "phases": ["atomic", "extended"],
      "context_window": 200000
    },
    "claude": {
      "api_base": "https://api.anthropic.com/v1",
      "model": "claude-sonnet-4-20250514",
      "env_key": "ANTHROPIC_API_KEY",
      "timeout": 120,
      "max_retries": 3,
      "retry_delay": 1.0,
      "rate_limit_rpm": 50,
      "phases": ["atomic", "extended"],
      "context_window": 200000
    },
    "claude_haiku": {
      "api_base": "https://api.anthropic.com/v1",
      "model": "claude-3-haiku-20240307",
      "env_key": "ANTHROPIC_API_KEY",
      "timeout": 60,
      "max_retries": 3,
      "retry_delay": 1.0,
      "rate_limit_rpm": 50,
      "phases": ["atomic"],
      "context_window": 200000
    },
    "openrouter": {
      "api_base": "https://openrouter.ai/api/v1",
      "model": "openrouter/auto",
      "env_key": "OPENROUTER_API_KEY",
      "timeout": 120,
      "max_retries": 3,
      "retry_delay": 1.0,
      "rate_limit_rpm": 60,
      "phases": ["atomic", "extended"]
    }
  },
  "timeout_settings": {
    "default_timeout_seconds": 120,
    "atomic_phase_timeout": 60,
    "extended_phase_timeout": 120,
    "fallback_timeout": 30
  },
  "retry_settings": {
    "max_retries": 3,
    "base_delay": 1.0,
    "max_delay": 30.0,
    "exponential_base": 2,
    "retry_on": ["timeout", "rate_limit", "server_error"]
  },
  "fallback_chain": {
    "primary": "minimax/MiniMax-M2.5",
    "fallback_1": "anthropic/claude-haiku",
    "fallback_2": "anthropic/claude-sonnet-4",
    "local_fallback": "lfm2.5-thinking:1.2b",
    "final_fallback": "mistral:7b"
  },
  "prompt_complexity": {
    "P1_P6": {
      "description": "Simple prompts - factual questions, short tasks",
      "target_model": "lfm2.5-thinking:1.2b",
      "phase": "atomic",
      "reason": "LFM fast for simple prompts, cost-effective"
    },
    "P7_P12": {
      "description": "Complex prompts - multi-step reasoning, analysis",
      "target_model": "anthropic/claude-sonnet-4",
      "phase": "extended",
      "reason": "Claude reliable for complex reasoning tasks"
    },
    "P13_P30": {
      "description": "Multi-turn conversations - context-heavy, ongoing",
      "target_model": "mistral:7b",
      "phase": "extended",
      "reason": "Mistral handles context well for multi-turn"
    }
  },
  "tier_1_online": [
    {
      "model": "minimax/MiniMax-M2.5",
      "priority": 1,
      "phases": ["atomic", "extended"],
      "api": "minimax",
      "reason": "200k context, reasoning, best overall"
    },
    {
      "model": "anthropic/claude-haiku",
      "priority": 2,
      "phases": ["atomic", "extended"],
      "api": "claude_haiku",
      "reason": "Fast, reliable for atomic tasks"
    },
    {
      "model": "anthropic/claude-sonnet-4",
      "priority": 3,
      "phases": ["atomic", "extended"],
      "api": "claude",
      "reason": "Higher capability for complex tasks"
    },
    {
      "model": "openai-codex/gpt-5.3-codex",
      "priority": 4,
      "phases": ["atomic", "extended"],
      "api": "openrouter",
      "reason": "Code-specific tasks"
    }
  ],
  "tier_2_local_fallback": [
    {
      "model": "lfm2.5-thinking:1.2b",
      "phases": ["atomic"],
      "enable_warmup": true,
      "fallback_model": "mistral:7b",
      "reason": "Best local for atomic when online unavailable"
    },
    {
      "model": "mistral:7b",
      "phases": ["extended"],
      "reason": "Fallback for extended when online unavailable"
    }
  ],
  "rules": [
    {
      "model": "lfm2.5-thinking:1.2b",
      "phase": "atomic",
      "enable_warmup": true,
      "fallback_model": "minimax/MiniMax-M2.5",
      "reason": "LFM as final fallback - online models preferred",
      "tier": 2,
      "source": "user_preference",
      "timestamp": 1771866000
    },
    {
      "model": "lfm2.5-thinking:1.2b",
      "phase": "extended",
      "enable_warmup": false,
      "fallback_model": "minimax/MiniMax-M2.5",
      "reason": "LFM doesn't handle extended - route to online",
      "tier": 2,
      "source": "user_preference",
      "timestamp": 1771866000
    },
    {
      "model": "lfm2.5-thinking:1.2b",
      "phase": "simple",
      "complexity": "P1_P6",
      "enable_warmup": true,
      "fallback_model": "anthropic/claude-haiku",
      "reason": "Simple prompts route to LFM for speed",
      "tier": 2,
      "source": "prompt_complexity_routing",
      "timestamp": 1737663180
    },
    {
      "model": "anthropic/claude-sonnet-4",
      "phase": "complex",
      "complexity": "P7_P12",
      "enable_warmup": false,
      "fallback_model": "minimax/MiniMax-M2.5",
      "reason": "Complex prompts route to Claude for reliability",
      "tier": 1,
      "source": "prompt_complexity_routing",
      "timestamp": 1737663180
    },
    {
      "model": "mistral:7b",
      "phase": "multi_turn",
      "complexity": "P13_P30",
      "enable_warmup": false,
      "fallback_model": "minimax/MiniMax-M2.5",
      "reason": "Multi-turn routes to Mistral for context handling",
      "tier": 2,
      "source": "prompt_complexity_routing",
      "timestamp": 1737663180
    },
    {
      "model": "lfm",
      "phase": "atomic",
      "enable_warmup": true,
      "fallback_model": null,
      "reason": "",
      "source": "test",
      "timestamp": 1771875464.8772895
    },
    {
      "model": "lfm",
      "phase": "extended",
      "enable_warmup": false,
      "fallback_model": "claude-haiku",
      "reason": "",
      "source": "test",
      "timestamp": 1771875464.8877249
    },
    {
      "model": "test-model",
      "phase": "atomic",
      "enable_warmup": true,
      "fallback_model": null,
      "reason": "test reason",
      "source": "harness_feedback",
      "timestamp": 1771875464.8958437
    }
  ]
}
