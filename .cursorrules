# AI Assistant Rules for Experiments Repository

> **Note:** These rules apply to ALL AI assistants working in this repository, including Cursor AI, Claude Code, and any other AI coding tools. See also `CLAUDE.md` for Claude Code-specific instructions.

## Project Structure
- All AI-generated artifacts and temporary files should be placed in `.cursor/artifacts/`
- Keep experimental projects organized in their respective folders (e.g., `mobile_experiments/Valdi/`)
- Maintain clean separation between source code and generated content

## Code Organization
- Follow platform-specific conventions (iOS/Swift, React Native, etc.)
- Keep experimental code well-documented with clear comments
- Use consistent naming conventions within each project

## File Management
- Never litter the root directory with generated files
- Place all temporary or generated artifacts in `.cursor/artifacts/`
- Keep README files updated with project status and setup instructions

## AI Assistant Guidelines
- When generating code, prefer creating new files over modifying existing ones unless explicitly requested
- Always check for existing similar implementations before creating new ones
- Document assumptions and limitations in code comments
- When unsure about framework specifics, create a flexible structure that can be adapted

## Valdi Framework Notes
- Valdi is Snapchat's cross-platform framework (iOS, Android, macOS) - NOT iOS-only
- Uses TypeScript/TSX with class-based components, lowercase tags (`<view>`, `<label>`), `onRender()` method
- Project structure requirements: `modules/my_app/src/` structure with `platforms/` folders and `WORKSPACE` file
- Import path: `valdi_core/src/Component` (verify against actual installation)
- CLI: `npm install -g @snap/valdi`, then `valdi dev_setup`, `valdi bootstrap`, `valdi install ios/android/macos`
- Official repo: https://github.com/Snapchat/Valdi (13k+ stars, active development)
- Always verify project structure matches Valdi requirements before proceeding

## Flutter Framework Notes
- Use `flutter create --org com.yourorg --project-name app_name` to scaffold proper project structure
- Always run `flutter pub get` after creating project or modifying pubspec.yaml
- Verify with `flutter analyze` before considering code complete
- Run `flutter test` to ensure widget tests pass
- Project structure must include: `pubspec.yaml`, `lib/`, `test/`, `android/`, `ios/` directories
- Default test file references `MyApp` - update when changing main app class name

## React Native Framework Notes
- Use `npx @react-native-community/cli init AppName --template @react-native-community/template` for TypeScript setup
- Verify with `npm run lint` and `npx tsc --noEmit` before considering code complete
- Run `npm test` to ensure Jest tests pass
- iOS requires macOS with Xcode and `pod install` in the ios/ directory
- Android requires Android Studio, SDK, and emulator setup
- Default test uses snapshot testing - update snapshots when UI changes: `npm test -- --updateSnapshot`

## Meta Learnings
- When working with new/experimental frameworks:
  1. Start with minimal viable structure
  2. Document assumptions clearly
  3. Create flexible boilerplates that can adapt
  4. Keep experimental code separate from production-ready code
  5. Always include setup instructions and next steps in README
  6. When documentation is unavailable, create comprehensive handoff documents:
     - Task lists with clear priorities
     - Understanding documents explaining research strategy
     - Documentation placeholders for discovered information
     - Next steps documents with actionable items
  7. For agent handoffs, create:
     - HANDOFF.md with quick start guide
     - TASKS.md with detailed task breakdown
     - UNDERSTANDING.md with context and research strategy
     - NEXT_STEPS.md with prioritized action items
  8. Document search attempts and results, even if unsuccessful
  9. Clearly mark placeholder code vs. actual framework code
  10. Set realistic expectations about framework availability

- Review and verification best practices:
  11. **Always verify project structure** against framework requirements - don't assume structure is correct just because code syntax looks right
  12. **Cross-reference documentation** - check for inconsistencies between different docs, and verify docs match actual project state
  13. **Verify import paths** - don't trust import statements without verifying they match actual framework installation
  14. **Conceptual execution** - mentally walk through setup/run steps to identify blockers before they're discovered
  15. **Multi-path analysis** - consider perspectives: developer (can I use this?), PM (risks?), reviewer (correctness?), user (does it work?), maintainer (maintainability?)
  16. **Structural verification checklist** - always verify: project structure matches requirements, import paths are correct, all components are used, required files exist
  17. **Dead code detection** - check if all code is actually used; unused code creates confusion
  18. **Documentation consistency** - check for outdated references, contradictory info, missing updates across all docs
  19. **Never trust without verification** - avoid “✅ done/verified” language unless you either (a) cite an official source, or (b) recorded the experiment results (commands + artifacts) in-repo
  20. **Think about runability** - code that looks correct but can't run is worse than no code

- Framework comparison best practices:
  21. **Scaffold before coding** - always use the framework's CLI to create proper project structure first, then add custom code; having code without scaffolding means nothing runs
  22. **Update test files with custom code** - default test files reference default class names (e.g., `MyApp`); always update tests when customizing the app
  23. **Parity implementation** - when comparing frameworks, implement identical features (same greeting, same interactions, same animations) for meaningful comparison
  24. **Multi-tool verification** - run ALL verification tools before declaring complete: linting (`flutter analyze`, `npm run lint`), type checking (`tsc --noEmit`), and tests (`flutter test`, `npm test`)
  25. **Documentation reflects reality** - update READMEs to reflect actual state (complete, tests passing) not planned state (planning only)
  26. **Headless environment awareness** - in CI/headless environments, verify code quality (lint, types, tests) even if you can't run simulators/emulators
  27. **Status tables are powerful** - use markdown tables to show at-a-glance status of multiple components across frameworks
  28. **Quick start commands** - every README should have copy-paste-able commands to get the app running
  29. **Version pinning** - document exact framework versions used (e.g., "Flutter 3.24.5", "React Native 0.82.1") for reproducibility
  30. **Comparison notes** - include explicit "vs. OtherFramework" sections to highlight differences for decision-making

- Document editing best practices:
  31. **Add columns, don't remove rows** - when adding new data to tables, add columns to existing tables rather than creating separate tables or removing existing structure
  32. **Never consolidate by deletion** - when asked to consolidate or make concise, ADD to existing structure rather than replacing/deleting valuable content
  33. **Preserve context sections** - sections like "Recommendations by Use Case", "Integration Advice", and "Data Sources" have value even if they seem verbose
  34. **Check git diff before major rewrites** - if making significant changes, verify what will be lost before overwriting
  35. **Concise ≠ shorter** - "concise" means removing redundancy, not removing information; a well-organized long document beats a short incomplete one
  36. **Table columns over separate tables** - integrate related data into one table with extra columns rather than creating multiple tables that fragment information
  37. **Verify data sources inline** - when adding data, include source links directly in the table legend or footnotes, not in separate sections that might get deleted

- Wallet comparison best practices:
  38. **Single unified table** - resist the urge to create multiple comparison tables; keep all data in one comprehensive table
  39. **Core criteria enforcement** - when comparing products/tools, define core criteria (e.g., "must have mobile + browser") and heavily penalize items that don't meet them
  40. **Scoring math must verify** - breakdown values MUST actually sum to the stated total score; verify with arithmetic
  41. **Values within bounds** - ensure no column exceeds its defined maximum (e.g., Security can't show 7/5)
  42. **Cross-document consistency** - when data exists in multiple places (main table, breakdown table, HTML), all must match
  43. **Release frequency as stability proxy** - for developer tools, lower release frequency often = more stability (inverse of typical "more updates = better")
  44. **Private repos ≠ unknown** - mark proprietary/private wallets appropriately; unknown release frequency (?) is different from inactive (❌)
  45. **Activity status decays** - what was active 6 months ago may be abandoned now; always verify with fresh data
  46. **No data loss on restructure** - when restructuring tables (adding columns, reorganizing), ALWAYS verify ALL original columns are preserved; use git diff to compare

- Data accuracy and anti-hallucination practices:
  47. **Chains ≠ tokens/assets** - "chains" means blockchain NETWORKS (Bitcoin, Ethereum, Solana = 3 chains), NOT total tokens/coins; a wallet supporting Ethereum + all ERC-20 tokens is still 1 chain with many tokens
  48. **Use categories when uncertain** - instead of hallucinating specific numbers like "9000+", use categories: "BTC" (Bitcoin only), "Multi" (multiple networks), or "verify on official site"
  49. **Approximate prices with ~** - prices change; use "~$150" not "$149" unless you just verified it; add "verify on official site" notes
  50. **Don't invent certifications** - EAL5+, EAL6+, EAL7 are specific Common Criteria certifications; don't claim them without verification; use chip names (ATECC, Optiga, ST33) when known
  51. **Add data accuracy disclaimers** - any comparison table should include a note that data changes and users should verify on official sites before purchasing
  52. **When uncertain, be vague not specific** - "Multi-chain" is honest; "5500+ chains" is a hallucination; being vague is better than being specifically wrong
  53. **Line-by-line verification** - before finalizing any data table, go through EVERY cell and ask "do I actually know this, or am I guessing?"
  54. **Distinguish marketing claims from facts** - manufacturers often inflate numbers (counting all tokens as "supported coins"); be skeptical and conservative

- Web analytics and third-party integrations:
  55. **Move decision artifacts to .cursor/artifacts/** - when creating comparison/decision documents (like analytics options), place them in `.cursor/artifacts/` not the project root; keep only essential implementation docs in the project
  56. **Static export compatibility** - for Next.js with `output: 'export'`, use client-side only implementations; avoid SSR-only packages like `@next/third-parties` which may not work with static exports
  57. **Environment variable fallbacks** - provide sensible defaults for analytics/config IDs in code, but allow override via env vars; document in `.env.example`
  58. **Client-side only components** - analytics scripts should be in `'use client'` components with `useEffect` hooks; check `typeof window !== 'undefined'` before accessing browser APIs
  59. **Clean up unused dependencies** - after implementation, remove any packages that were installed but not used (e.g., if you try `@next/third-parties` but use custom implementation instead)
  60. **Update references when moving files** - when moving artifacts to `.cursor/artifacts/`, search for and update all references in README/docs to avoid broken links
  61. **Verify build after cleanup** - always run `npm run build`, `npm run type-check`, and `npm run lint` after removing dependencies or moving files to ensure nothing breaks

- Web scraping and data verification:
  62. **Use browser automation for Cloudflare-protected sites** - when encountering Cloudflare protection (showing "Just a moment..." pages), use browser automation tools like Playwright or Puppeteer instead of asking users to manually fetch information. Install with `npm install playwright` and `npx playwright install chromium`, then use headless mode with appropriate wait times (30+ seconds) for Cloudflare challenges to complete. Never ask users to manually scrape sites - use available tools to do it yourself.
  63. **Try multiple scraping approaches** - if curl fails, try: 1) Better user agents and headers, 2) Browser automation (Playwright/Puppeteer), 3) Python requests with BeautifulSoup, 4) Wait longer for JavaScript-heavy pages. Always attempt automated solutions before requesting manual help.
  64. **Document verification methods** - when verifying information via browser automation or other tools, document the method used (e.g., "Verified via Playwright browser automation") and the date in verification notes. This helps future verification efforts understand what worked.

- SEO and social media card best practices:
  65. **Static vs dynamic OG images** - before adding OG/Twitter card metadata, check existing pattern: does the project use static pre-generated PNGs or dynamic API-generated cards? Follow the established pattern.
  66. **Consistency across pages** - when adding SEO to a new page, copy the EXACT pattern from similar existing pages: same ogImageVersion, same Twitter handles (@creator, @site), same image dimensions (1200x630), same metadata structure.
  67. **Static pages hardcode, dynamic pages use functions** - in Next.js apps, static pages (layout, /explore) often hardcode image paths directly, while dynamic routes (/docs/[slug]) use lookup functions. Don't add entries to lookup functions for static pages - it creates dead code.
  68. **Cache-busting version parameter** - OG images should include version parameters (e.g., `?v3`) to bust social media caches when images are updated. Increment the version when regenerating images.
  69. **Verify all metadata fields** - Twitter cards need: card type, title, description, creator, site, images. OG needs: title, description, url, type, images (with width/height/alt). Missing fields = incomplete social previews.
  70. **Regenerate OG images when adding pages** - when adding a new page to wallets/frontend that needs an OG image: 1) Add a generate function to `scripts/generate-og-images.js`, 2) Run `npm run generate-og`, 3) Add metadata to the page pointing to the new image, 4) Commit the generated PNG. CI will fail if images are out of sync.
  71. **Increment ogImageVersion on image updates** - when OG images are regenerated, increment the version parameter (e.g., `v3` → `v4`) in layout.tsx and affected pages to bust social media caches.

- Token economics and financial projections:
  72. **Calculate token price from primary data** - always calculate token price from verified fundraising data (amount raised ÷ tokens sold), not from third-party listing sites which may have outdated or incorrect prices
  73. **Verify fundraising math** - when sources conflict on token price, trust the calculation from official fundraising data: if they raised $61M selling 1.547B tokens, the price is $0.0394 regardless of what ICO listing sites claim
  74. **FDV is floor price in auctions** - in token auctions, the announced FDV represents the starting floor price; actual clearing price may be higher; calculate from actual amount raised for accuracy
  75. **Price errors compound in projections** - a small token price error (e.g., $0.0464 vs $0.04 = 16% error) propagates through ALL TVL and revenue calculations; always verify base assumptions first
  76. **Multi-pass financial review required** - when user says "numbers don't look right," they're usually correct; systematically verify: 1) token price from primary sources, 2) all TVL calculations, 3) revenue formulas, 4) break-even math, 5) cross-check against known data points
  77. **Consolidate financial docs to single source** - having the same revenue projections in 3+ documents creates maintenance burden and inconsistency; consolidate to one ECONOMICS.md file and reference it from executive summary/assumptions
  78. **Show calculation methodology** - always show formulas (e.g., "TVL × 8% APY × 10% fee = revenue") not just final numbers; makes errors easier to spot and builds trust
  79. **Conservative estimates for new protocols** - for protocols without proven track record, use conservative estimates: token price at sale price (not projected), APY at sustainable levels (not bootstrap), market share at 30% (not 50%)
  80. **Flag non-transferable tokens** - if tokens aren't tradable yet (locked until TGE), explicitly note this; it's a major risk factor that affects price assumptions
  81. **Revenue in protocol token, costs in fiat** - clarify that liquid staking revenue is in protocol tokens (scales with price) while infrastructure costs are in fiat (fixed); this asymmetry is critical for sensitivity analysis

## Best Practices
- Use semantic versioning for packages
- Include proper .gitignore files for each project type
- Maintain clear separation between different experiments
- Document tooling requirements and setup steps
- Keep dependencies minimal for experimental projects
